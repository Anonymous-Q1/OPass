#[version = "0.0.5"]
def @main(%x0: Tensor[(1, 64, 56, 56), float32], %x1: Tensor[(64, 64, 3, 3), float32]) {
    %0 = nn.global_max_pool2d(%x0, layout="NCHW", out_layout="");
    %1 = multiply(%0, 2.0f);
    %2 = (%0, %1);
    %3 = exp(2.0f);
    %4 = nn.batch_to_space_nd(%x0, block_shape=[2, 2], crops=[[0, 1], [0, 1]]);
    %5 = nn.relu(%4);
    %6 = transpose(%5, axes=[0, 2, 3, 1]);
    %7 = transpose(%6, axes=[1, 2, 3, 0]);
    %8 = transpose(%7, axes=[3, 2, 0, 1]);
    %9 = ones(shape=[1], dtype="float32");
    %10 = add(%1, %9);
    %11 = nn.conv2d(%10, %x1, strides=[1, 1], padding=[1, 1, 1, 1], dilation=[1, 1], groups=1, channels=64, kernel_size=[3, 3], data_layout="NCHW", kernel_layout="OIHW", out_layout="", out_dtype="");
    %12 = nn.relu(%11);
    %13 = nn.global_max_pool2d(%6, layout="NCHW", out_layout="");
    %14 = nn.relu(%13);
    %15 = transpose(%14, axes=[0, 2, 3, 1]);
    %16 = transpose(%15, axes=[1, 2, 3, 0]);
    %17 = transpose(%16, axes=[3, 2, 0, 1]);
    %18 = layout_transform(%7, src_layout="NCHW", dst_layout="NCHW4c");
    %19 = layout_transform(%18, src_layout="NCHW4c", dst_layout="NCHW");
    %20 = nn.relu(%19);
    %21 = (%2, %3, %8, %12, %17, %20);
    %21
}
