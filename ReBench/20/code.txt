#[version = "0.0.5"]
def @main(%x0: Tensor[(1, 4, 2), int8], %x1: Tensor[(1, 4, 2), int8], %x2: Tensor[(31, 65), int64], %x3: Tensor[(58,), float32], %x4: Tensor[(1, 100, 2), int8], %x5: Tensor[(1, 18, 2), int8], %x6: Tensor[(1, 46, 2), int8], %x7: Tensor[(49, 17, 31), int32]) {
    %0 = qnn.dequantize(%x0, 2.0f, 0, axis=-1);
    %1 = qnn.dequantize(%x1, 6.0f, 0, axis=-1);
    %2 = nn.batch_matmul(%0, %1, out_dtype="", transpose_a=0, transpose_b=1);
    %3 = add(%2, 1.0f);
    %4 = erf(%3);
    %5 = cast(%x1, dtype="int16");
    %6 = cast(%5, dtype="int32");
    %7 = cast_like(%6, %x2);
    %8 = cast_like(%7, %x3);
    %9 = qnn.dequantize(%x0, 2.0f, 0, axis=-1);
    %10 = qnn.dequantize(%x4, 6.0f, 0, axis=-1);
    %11 = nn.batch_matmul(%9, %10, out_dtype="", transpose_a=0, transpose_b=1);
    %12 = cast(%x2, dtype="bool");
    %13 = cast(%12, dtype="int32");
    %14 = qnn.dequantize(%x4, 2.0f, 0, axis=-1);
    %15 = qnn.dequantize(%x5, 0.5f, 0, axis=-1);
    %16 = nn.batch_matmul(%14, %15, out_dtype="", transpose_a=0, transpose_b=1);
    %17 = qnn.quantize(%16, 1.0f, 0, out_dtype="int8", axis=-1);
    %18 = ones(shape=[1], dtype="float32");
    %19 = add(%1, %18);
    %20 = qnn.dequantize(%x5, 2.0f, 0, axis=-1);
    %21 = qnn.dequantize(%x6, 0.5f, 0, axis=-1);
    %22 = nn.batch_matmul(%20, %21, out_dtype="", transpose_a=0, transpose_b=1);
    %23 = qnn.quantize(%22, 1.0f, 0, out_dtype="int8", axis=-1);
    %24 = multiply(%18, %18);
    let %x8 = %24;
    %25 = zeros_like(%18);
    %26 = ones_like(%x8);
    let %x9 = %26;
    %27 = multiply(%x9, %18);
    %28 = collapse_sum_like(%27, %18);
    %29 = add(%25, %28);
    %30 = multiply(%x9, %18);
    %31 = collapse_sum_like(%30, %18);
    %32 = add(%29, %31);
    %33 = (%32,);
    %34 = (%x8, %33);
    %35 = qnn.dequantize(%x6, 2.0f, 0, axis=-1);
    %36 = argmax(%35, axis=[1], keepdims=0, select_last_index=0, exclude=0);
    %37 = cast_like(%16, %x7);
    %38 = (%4, %8, %11, %13, %17, %19, %23, %34, %36, %37);
    %38
}
