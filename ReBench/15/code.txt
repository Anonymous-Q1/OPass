#[version = "0.0.5"]
def @main(%x0: Tensor[(1, 224, 224, 3), int8], %x1: Tensor[(16, 3, 5, 5), int8], %x2: Tensor[(16,), int32], %x3: Tensor[(84,), int32]) {
    %0 = qnn.dequantize(%x0, 2.0f, 0, axis=-1);
    %1 = transpose(%0, axes=[0, 3, 1, 2]);
    %2 = qnn.dequantize(%x1, 0.5f, 0, axis=-1);
    %3 = nn.conv2d(%1, %2, strides=[1, 1], padding=[0, 0, 0, 0], dilation=[1, 1], groups=1, channels=None, kernel_size=[5, 5], data_layout="NCHW", kernel_layout="OIHW", out_layout="", out_dtype="");
    %4 = qnn.dequantize(%x2, 2.0f, 0, axis=-1);
    %5 = nn.bias_add(%3, %4, axis=1);
    %6 = qnn.quantize(%5, 1.0f, 0, out_dtype="int8", axis=-1);
    %7 = qnn.dequantize(%x1, 2.0f, -12, axis=-1);
    %8 = nn.avg_pool2d(%7, pool_size=[3, 3], strides=[1, 1], dilation=[1, 1], padding=[0, 0, 0, 0], layout="NCHW", out_layout="", ceil_mode=0, count_include_pad=0);
    %9 = qnn.quantize(%8, 0.5f, 10, out_dtype="int8", axis=-1);
    let %x4 = %x2;
    %10 = shape_of(%5, dtype="int32");
    %11 = dyn.ones(%10, shape=None, dtype="float32");
    let %x5 = %x3;
    %12 = zeros_like(%x5);
    %13 = nn.global_max_pool2d(%1, layout="NCHW", out_layout="");
    %14 = nn.softmax(%4, axis=-1);
    %15 = cast(%14, dtype="float16");
    %16 = qnn.dequantize(%x0, 0.1f, 10, axis=-1);
    %17 = multiply(%16, 1.0f);
    %18 = qnn.quantize(%17, 0.1f, 10, out_dtype="int8", axis=-1);
    %19 = sqrt(%7);
    %20 = divide(2.0f, %19);
    %21 = add(2.0f, 2.0f);
    %22 = (%6, %9, %x4, %11, %12, %13, %15, %18, %20, %21);
    %22
}
